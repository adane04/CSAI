{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d29d674-9cd3-4dfe-adf7-26e8f1b4a9a0",
   "metadata": {},
   "source": [
    "### Continued from the preprocessing_GDELT ----notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31867c-b4f4-455c-a0af-7342cc74e846",
   "metadata": {},
   "source": [
    "\n",
    "********************************************************************************************************************\n",
    "********************************************************************************************************************\n",
    "#### Continued from the preprocessing_GDELT ----notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0c38474-15a6-4bc5-a1cc-abf273e52b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee0b8891-fea3-4403-90d5-7f0e49a05160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17052, 19)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
 
    "df = pd.concat([keys_df]*1, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "533ca080-81e1-45e3-af39-d4efb5fca989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into Training and Testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "train,test = train_test_split(df,test_size=0.20)\n",
    "X_train=train.loc[:,['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7','d8', 'd9', 'd10','Embeddings','key_umap']]\n",
    "X_test=test.loc[:,['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7','d8', 'd9', 'd10','Embeddings','key_umap']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d38d1a-fba6-43d8-8305-eb22dda1a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize clustering and partitioning\n",
    "kmeans = KMeans(n_clusters=7,init='k-means++', max_iter=300, n_init=10) \n",
    "cv = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "counter=0\n",
    "overall_CSAI_all_samples=[]\n",
    "\n",
    "# Now, let's keep only X_train for further partitioning\n",
    "for train_index, _ in cv.split(X_train):\n",
    "    X_train_prt = X_train.iloc[train_index]\n",
    "    #print(X_train_partitioned)\n",
    " \n",
 
    "\n",
    "    #print(X_train.shape)\n",
    "    # Take only the scores/features for both testing and training\n",
    "    X_train_prt=X_train_prt[['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7','d8', 'd9', 'd10','Embeddings','key_umap']]\n",
    "    X_test=X_test[['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7','d8', 'd9', 'd10','Embeddings','key_umap']]\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    embedding_values_array = np.array(X_train_prt[\"key_umap\"].tolist())\n",
    "    \n",
    "    # Reduce dimensionality using UMAP\n",
    "    umap_embedding = umap.UMAP().fit_transform(embedding_values_array)\n",
    "    \n",
    "    # Apply KMeans clustering with UMAP\n",
    "    #kmeans = KMeans(n_clusters=5,init='k-means++', max_iter=300, n_init=10)  # You may adjust the number of clusters as needed\n",
    "    labels = kmeans.fit_predict(umap_embedding)\n",
    "    X_train_prt[\"Cluster\"] = [str(label) for label in labels]\n",
    "    #print(f\"Num of clusters: {len(np.unique(labels))}\")\n",
    "    \n",
    "    # Model fit with Testing data\n",
    "    embedding_values_array_test = np.array(X_test[\"key_umap\"].tolist())\n",
    "    # Reduce dimensionality using UMAP\n",
    "    umap_embedding_test = umap.UMAP().fit_transform(embedding_values_array_test)\n",
    "    y_pred_test=kmeans.predict(umap_embedding_test)\n",
    "    X_test[\"Cluster\"] = [str(label) for label in y_pred_test]\n",
    "    #print(f\"Num of clusters: {len(np.unique(labels))}\")  \n",
    "\n",
    "    # Calculate the sum of values in \"key_umap\" for each row\n",
    "    #X_train_prt['sum_umap_train'] = X_train_prt['key_umap'].apply(np.sum)\n",
    "    #X_test['sum_umap_test'] = X_test['key_umap'].apply(np.sum)\n",
    "\n",

    "    \n",
    "    # incude the sum of key_umap as new column\n",
    "    Xtrain=X_train_prt[['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7','d8', 'd9', 'd10','Cluster']]\n",
    "    Xtest=X_test[['d1', 'd2', 'd3', 'd4', 'd5', 'd6', 'd7','d8', 'd9', 'd10','Cluster']]\n",
    "\n",
    "    #Train :  group and sum values of each cluster on train\n",
    "    Xtrain=Xtrain.groupby(['Cluster']).sum()\n",
    "    Xtrain2=Xtrain.T\n",
    "    Xtrain2.columns=['c0', 'c1', 'c2','c3', 'c4', 'c5', 'c6'];\n",
    "    Xtrain3=Xtrain2.T\n",
    "    \n",
    "    #Total sum of values in all clusters on train\n",
    "    #Xtrain3_sums = Xtrain3.sum(axis=0)\n",
    "\n",
    "    # Test: group and sum values of each cluster  on test\n",
    "    Xtest=Xtest.groupby(['Cluster']).sum()\n",
    "    Xtest2=Xtest.T\n",
    "    #Xtest2.columns=['c0', 'c1', 'c2','c3', 'c4'];\n",
    "    Xtest3=Xtest2.T\n",
    "    #Xtest3=Xtest3[\"sum_umap_test\"];Xtest3\n",
    "    \n",
    "    # Train:Total sum of values in all clusters on train\n",
    "    Xtrain3_sums = Xtrain3.sum(axis=0)\n",
    "    # Train: Normalize values by dividing the values of each cluster by the total number \n",
    "    ct0=pd.Series(Xtrain3.iloc[0]/Xtrain3_sums)\n",
    "    ct1=pd.Series(Xtrain3.iloc[1]/Xtrain3_sums)\n",
    "    ct2=pd.Series(Xtrain3.iloc[2]/Xtrain3_sums)\n",
    "    ct3=pd.Series(Xtrain3.iloc[3]/Xtrain3_sums)\n",
    "    ct4=pd.Series(Xtrain3.iloc[4]/Xtrain3_sums)\n",
    "    ct5=pd.Series(Xtrain3.iloc[5]/Xtrain3_sums)\n",
    "    ct6=pd.Series(Xtrain3.iloc[6]/Xtrain3_sums)\n",
    "    \n",
    "    #Join clusters as columns and rename\n",
    "    df_per_train=pd.concat([ct0,ct1,ct2,ct3,ct4,ct5,ct6],axis=1)\n",
    "    df_per_train.columns = ['c0', 'c1', 'c2','c3', 'c4', 'c5', 'c6']\n",
    "    #df_per_test=df_per_train.T\n",
    "    df_per_train\n",
    "\n",
    "   \n",
    "    # Test: Total sum of values in all clusters on Test\n",
    "    Xtest3_sums = Xtest3.sum(axis=0)\n",
    "    # Normalize values by dividing the values of each cluster by the total number \n",
    "    c00=pd.Series(Xtest3.iloc[0]/Xtest3_sums)\n",
    "    c10=pd.Series(Xtest3.iloc[1]/Xtest3_sums)\n",
    "    c20=pd.Series(Xtest3.iloc[2]/Xtest3_sums)\n",
    "    c30=pd.Series(Xtest3.iloc[3]/Xtest3_sums)\n",
    "    c40=pd.Series(Xtest3.iloc[4]/Xtest3_sums)\n",
    "    c50=pd.Series(Xtest3.iloc[5]/Xtest3_sums)\n",
    "    c60=pd.Series(Xtest3.iloc[6]/Xtest3_sums)\n",
    "\n",
    "    #Join clusters as columns and rename\n",
    "    df_per_test=pd.concat([c00,c10,c20,c30,c40,c50,c60],axis=1)\n",
    "    df_per_test.columns = ['c0', 'c1', 'c2','c3', 'c4', 'c5', 'c6']\n",
    "    #df_per_test=df_per_train.T\n",
    "    df_per_test\n",
    "\n",
    "    # Define overall RMSE variable before the loop\n",
    "   \n",
    "  \n",
    "   # Calculate the range of the values  for train\n",
    "    range_y = np.max(df_per_train) - np.min(df_per_train)\n",
    "    overall_rmse_per_cluster = []\n",
    "    \n",
    "    # Calculate RMSE for each cluster \n",
    "    for i in range(3):\n",
    "        #train_df = train_dfs[i]\n",
    "        #test_df = test_dfs[i]\n",
    "        \n",
    "        # Calculate the squared errors for each cluster in each partition\n",
    "        #squared_errors = (sums_df1_cluster.values - sums_df2_cluster.values) ** 2\n",
    "        squared_errors = (df_per_train.iloc[:, i] - df_per_test.iloc[:, i]) ** 2\n",
    "        # Calculate the mean squared error for each cluster in each partition\n",
    "        mean_squared_error = np.mean(squared_errors)\n",
    "        # Calculate the root mean squared error (RMSE) for each cluster in each partition\n",
    "        rmse_per_cluster = np.sqrt(mean_squared_error)/range_y\n",
    "        #print(f\"RMSE for partition {i+1} for each cluster:\", rmse_per_cluster)\n",
    "        # Append the RMSE for this partition to the overall RMSE list\n",
    "        overall_rmse_per_cluster.append(rmse_per_cluster)\n",
    "        \n",
    "        \n",
    "    print(\"Sample:\",counter)\n",
    "    print(\"Training set:\",X_train.shape)\n",
    "    print(f\"Num of clusters: {len(np.unique(labels))}\") \n",
    "    counter += 1\n",
    "        \n",
    "    # Convert the overall RMSE list to a numpy array\n",
    "    overall_rmse_per_cluster = np.array(overall_rmse_per_cluster)\n",
    "    \n",
    "    # Calculate the overall RMSE by averaging the RMSEs of all clusters across all partitions\n",
    "    overall_rmse = np.mean(overall_rmse_per_cluster)\n",
    "    overall_CSAI_all_samples.append(overall_rmse)\n",
    "    \n",
    "    #print(\"\\nOverall RMSE for each cluster:\", overall_rmse_per_cluster)\n",
    "    #print(\"\\nOverall RMSE across all clusters and partitions:\", overall_rmse)\n",
    "    print(\"\\n CSAI per cluster in this partition:\", [round(val, 4) for val in overall_rmse_per_cluster])\n",
    "    print(\"\\n  CSAI in this  partition :\", round(overall_rmse, 4))\n",
    "    print(\"\\n\")\n",
    "\n",
    "overall_CSAI = np.mean(overall_CSAI_all_samples)\n",
    "print(\"\\nOverall CSAI across all samples :\", round(overall_CSAI, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce9b4a-22a2-4fca-8ac2-73e573bdfe51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202a945-921c-4126-8b56-e00b5cf0be5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
